{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "XcBbWbfeY83Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "09ClfZVYZDwy"
      },
      "outputs": [],
      "source": [
        "def train_best_model():\n",
        "    # Load the data\n",
        "    data = pd.read_csv('sample_data/data_model.csv', encoding='ISO-8859-1')\n",
        "\n",
        "    # Map actual labels to class labels\n",
        "    class_mapping = {'d': 0, 'l': 1, 'w': 2}\n",
        "    data['RESULTS'] = data['RESULTS'].map(class_mapping)\n",
        "\n",
        "    # Preprocess the data\n",
        "    features = data[['W', 'D', 'L']]\n",
        "    target = data['RESULTS']\n",
        "\n",
        "    # Initialize variables\n",
        "    best_accuracy = 0.0\n",
        "    best_random_state = None\n",
        "    max_iterations = 200  # You can adjust this value\n",
        "    iteration = 0\n",
        "\n",
        "    while iteration < max_iterations:\n",
        "        # Split the data into training and testing sets with a different random state each time\n",
        "        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=iteration + 1, stratify=target)\n",
        "\n",
        "        # Standardize features\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "        # Define the XGBoost model\n",
        "        model = xgb.XGBClassifier(\n",
        "            objective='multi:softmax',\n",
        "            num_class=5,\n",
        "            early_stopping_rounds=15,\n",
        "            reg_lambda=0.2,\n",
        "            learning_rate=0.1,\n",
        "            n_estimators=1000,\n",
        "            tree_method='hist',\n",
        "            device='cuda',\n",
        "            eval_metric='merror'\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train,  eval_set=[(X_test, y_test)])\n",
        "\n",
        "        # Make predictions on the test set\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Evaluate the model\n",
        "        current_accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Iteration {iteration + 1}: Accuracy = {current_accuracy * 100:.2f}%\")\n",
        "\n",
        "        # Update the best accuracy and random state if the current accuracy is higher\n",
        "        if current_accuracy > best_accuracy:\n",
        "            best_accuracy = current_accuracy\n",
        "            best_random_state = iteration + 1\n",
        "            best_model = model  # Save the best model\n",
        "            best_scaler = scaler  # Save the best scaler\n",
        "\n",
        "        # Update iteration count\n",
        "        iteration += 1\n",
        "\n",
        "    # Print the final result\n",
        "    if best_accuracy >= 60.0:\n",
        "        print(f\"Highest Accuracy: {best_accuracy * 100:.2f}% (Random State: {best_random_state})\")\n",
        "    else:\n",
        "        print(f\"No iteration achieved 60% accuracy. Best Accuracy: {best_accuracy * 100:.2f}% (Random State: {best_random_state})\")\n",
        "\n",
        "    return best_model, best_scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHF6ohAbZEdS"
      },
      "outputs": [],
      "source": [
        "# Call the function to get the best model and scaler\n",
        "best_model, best_scaler = train_best_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DABibWOZwgyf",
        "outputId": "94c4e001-8cb0-4d75-9eb5-ef639986847b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Outcome: w\n"
          ]
        }
      ],
      "source": [
        "# Now you can use the best model and scaler for predictions on new data\n",
        "new_match = pd.DataFrame({'W': [2.00], 'D': [3.44], 'L': [3.92]})\n",
        "new_match_standardized = best_scaler.transform(new_match)\n",
        "new_prediction = best_model.predict(new_match_standardized)\n",
        "predicted_outcome = [k for k, v in class_mapping.items() if v == new_prediction][0]\n",
        "print(\"Predicted Outcome:\", predicted_outcome)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}